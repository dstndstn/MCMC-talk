{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulated-mixture",
   "metadata": {},
   "source": [
    "## Affine-Invariant Monte Carlo sampling\n",
    "\n",
    "Today we'll experiment with a Julia implementation of an Affine Invariant sampler (similar to emcee in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "]add AffineInvariantMCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "using AffineInvariantMCMC\n",
    "using WGLMakie\n",
    "using Optim\n",
    "using ForwardDiff\n",
    "using LinearAlgebra\n",
    "using LogExpFunctions\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our good old data set\n",
    "# Example data set from  arxiv:1008.4686, table 1 (https://arxiv.org/abs/1008.4686)\n",
    "# You can also refer to that paper for more background, equations, etc.\n",
    "alldata = [201. 592 61; 244 401 25; 47  583 38; 287 402 15; 203 495 21; 58  173 15; 210 479 27;\n",
    "           202 504 14; 198 510 30; 158 416 16; 165 393 14; 201 442 25; 157 317 52; 131 311 16;\n",
    "           166 400 34; 160 337 31; 186 423 42; 125 334 26; 218 533 16; 146 344 22 ]\n",
    "# The first 5 data points are outliers; for the first part we'll just use the \"good\" data points\n",
    "x    = alldata[6:end, 1]\n",
    "y    = alldata[6:end, 2]\n",
    "# this is the standard deviation (uncertainty) on the y measurements, also known as \\sigma_i\n",
    "yerr = alldata[6:end, 3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_likelihood_one(params, x, y, yerr)\n",
    "    \"\"\"This function computes the log-likelihood of a data set with coordinates\n",
    "    (x_i,y_i) and Gaussian uncertainties on y_i of yerr_i (aka sigma_i)\n",
    "\n",
    "    The model is a straight line, so the model's predicted y values are\n",
    "        y_pred_i = b + m x_i.\n",
    "\n",
    "    params = (b,m) are the parameters (scalars)\n",
    "    x,y,yerr are arrays (aka vectors)\n",
    "\n",
    "    Return value is a scalar log-likelihood.\n",
    "    \"\"\"\n",
    "    # unpack the parameters\n",
    "    b,m = params\n",
    "    # compute the vector y_pred, the model predictions for the y measurements\n",
    "    y_pred = b .+ m .* x\n",
    "    # compute the log-likelihoods for the individual data points\n",
    "    # (the quantity inside the sum in the text above)\n",
    "    loglikes = log.(1 ./ (sqrt(2*Ï€) .* yerr)) .- 0.5 .*(y - y_pred).^2 ./ yerr.^2\n",
    "    # the log-likelihood for the whole vector of measurements is the sum of individual log-likelihoods\n",
    "    loglike = sum(loglikes)\n",
    "    return loglike\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the sampler a number of things:\n",
    "# - how many dimensions are being sampled\n",
    "numdims = 2\n",
    "# - how many walkers we want\n",
    "numwalkers = 100\n",
    "# - \"thinning\" means: the sampler only records the samples after every N-th step.  That is, with thinning=10, it takes\n",
    "#   10 steps and then saves the walker positions.  Since we know the samples can be correlated, this leaves us a smaller\n",
    "#   set of samples that are less correlated.\n",
    "thinning = 10\n",
    "# - how many steps to take -- the number of samples we'll get out is this value divided by thinning.\n",
    "numsamples_perwalker = 2000\n",
    "# In the example code below, it runs an initial \"burn-in\" round of sampling, then does the \"real\" sampling.  This is how many\n",
    "# burn-in samples to take.\n",
    "burnin = 100\n",
    "\n",
    "# We also need to give the walkers some initial positions.  NOTE that you can't give them all the same position!\n",
    "# Here, we're just drawing uniform numbers of between 0 and 1.  That'll work fine.\n",
    "x0 = rand(numdims, numwalkers) .* 1\n",
    "\n",
    "# We need to pass to the sampler a function that takes *only* the vector of parameters.  Our log_likelihood_one function\n",
    "# also needs the (x,y,yerr) values.  So to make this work, we need a \"wrapper\" function to can grab the (x,y,yerr) values\n",
    "# and pass them to log_likelihood_one.\n",
    "\n",
    "ll_func(bm) = log_likelihood_one(bm, x, y, yerr)\n",
    "\n",
    "# Here we go, let's call the AffineInvariant MCMC's \"sample\" function -- this is the burn-in round!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, x0, burnin, 1)\n",
    "\n",
    "# And here's the \"real\" run.\n",
    "# Notice that it's passing in the end of the burn-in chain as the initial position!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, chain[:, :, end], numsamples_perwalker, thinning)\n",
    "\n",
    "# And that's it!  Now \"chain\" contains our samples!\n",
    "println(\"Chain:\", size(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that \"chain\" is a three-dimensional vector -- the size being\n",
    "#  (number of dimensions, number of walkers, number of thinned samples)\n",
    "\n",
    "# There is a handy function to \"flatten\" that array, because we often don't care which walker generated the\n",
    "# samples.\n",
    "\n",
    "flatchain, flat_llvals = AffineInvariantMCMC.flattenmcmcarray(chain, llvals)\n",
    "println(\"Flatchain:\", size(flatchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also note those \"llvals\" -- those are the log-likelihood values corresponding to the parameter values (b,m) in the chain.\n",
    "# We don't really need those for what we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's call our old friend the optimizer to get its assessment of the uncertainty ellipse --\n",
    "initial_params = [0., 0.]\n",
    "result = optimize(p -> -log_likelihood_one(p, x, y, yerr), initial_params)\n",
    "b_ml,m_ml = Optim.minimizer(result)\n",
    "invhess = inv(ForwardDiff.hessian(p -> -log_likelihood_one(p, x, y, yerr), [b_ml,m_ml]))\n",
    "S = svd(invhess)\n",
    "SS = S.U * Diagonal(sqrt.(S.S))\n",
    "th = LinRange(0., 2Ï€, 200)\n",
    "xx = sin.(th)\n",
    "yy = cos.(th)\n",
    "dbm = SS * [xx yy]'\n",
    "ellipse_b = b_ml .+ dbm[1,:]\n",
    "ellipse_m = m_ml .+ dbm[2,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Figure()\n",
    "Axis(f[1,1], xlabel=\"B\", ylabel=\"M\")\n",
    "s = WGLMakie.scatter!(flatchain[1,:], flatchain[2,:])\n",
    "lines!(ellipse_b, ellipse_m, color=:red)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey, that looks pretty good!  And no step sizes to fiddle with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a clearer view of the distribution, we can use \"hexbin\" to make a histogram,\n",
    "f = Figure()\n",
    "Axis(f[1,1], xlabel=\"B\", ylabel=\"M\")\n",
    "s = WGLMakie.hexbin!(flatchain[1,:], flatchain[2,:], bins=100)\n",
    "lines!(ellipse_b, ellipse_m, color=:red)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-harvest",
   "metadata": {},
   "source": [
    "## Affine Invariant Sampler vs Multi-modal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-daughter",
   "metadata": {},
   "source": [
    "Let's have a look at how the sampler breaks down when given a multi-modal distribution.\n",
    "\n",
    "We'll cook up a multi-modal likelihood function made up of two Gaussians placed at different points in an $a,b$ plane that we'll sample in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multi-modal distribution, with 2 parameters.\n",
    "function log_like_multimodal(params)\n",
    "    # unpack the parameters\n",
    "    a,b = params\n",
    "\n",
    "    # The distribution has two Gausians centered at these positions and with unit variance.\n",
    "    # peak 1\n",
    "    a1,b1 = 0., 5.\n",
    "    # peak 2\n",
    "    a2,b2 = 10., 5.\n",
    "    # The fraction of mass in peak 1; peak2 will have 1-frac1\n",
    "    frac1 = 0.5\n",
    "    @assert frac1 >= 0\n",
    "    @assert frac1 <= 1\n",
    "    frac2 = 1 - frac1\n",
    "\n",
    "    # I'm ignoring the 1/(sqrt(2pi) sigma) term of the Gaussian probability function \n",
    "    # because that is constant, and the sampler only cares about *relative*\n",
    "    # changes in the log-likelihood.\n",
    "    \n",
    "    loglike1 = - 0.5 .* ((a - a1).^2 + (b - b1).^2)\n",
    "    loglike2 = - 0.5 .* ((a - a2).^2 + (b - b2).^2)\n",
    "    # the log-likelihood for the whole vector of measurements is the sum of individual weighted log-likelihoods\n",
    "    loglike = sum(logaddexp(log(frac1) + loglike1, log(frac2) + loglike2))\n",
    "    return loglike\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sampler on this multi-modal distribution!\n",
    "numdims = 2\n",
    "numwalkers = 100\n",
    "thinning = 1\n",
    "numsamples_perwalker = 2000\n",
    "burnin = 100\n",
    "# Random initial positions for the walkers, in [0, 1)\n",
    "x0 = rand(numdims, numwalkers) .* 1\n",
    "chain, llhoodvals = AffineInvariantMCMC.sample(log_like_multimodal, numwalkers, x0, burnin, 1)\n",
    "chain, llhoodvals = AffineInvariantMCMC.sample(log_like_multimodal, numwalkers, chain[:, :, end], numsamples_perwalker, thinning)\n",
    "println(\"Chain:\", size(chain))\n",
    "flatchain, flatllhoodvals = AffineInvariantMCMC.flattenmcmcarray(chain, llhoodvals)\n",
    "println(\"Flatchain:\", size(flatchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Figure()\n",
    "Axis(f[1,1], xlabel=\"A\", ylabel=\"B\")\n",
    "s = WGLMakie.hexbin!(flatchain[1,:], flatchain[2,:], bins=100)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-expense",
   "metadata": {},
   "source": [
    "So that's kind of interesting, isn't it??  The sampler *has* explored both peaks of the distribution, but not equally!\n",
    "\n",
    "<b>Experiment with this a bit.  Try re-running it a few times -- do you always get the same split between the two peaks?</b>\n",
    "    \n",
    "<b>What if you run for more samples?</b>\n",
    "\n",
    "<b>Try moving the initialization point -- does that change the balance between the two peaks?</b>\n",
    "\n",
    "<b>Try moving the centers of the peaks in the <tt>log_like_multimodal</tt> function.  How close together do they have to be for you to often get them looking balanced?</b>\n",
    "\n",
    "<b>Try one extra thing -- what if you make it easier to \"wander\" between the peaks?  That is, like we did when we were allowing outliers in our fitting, what if we add a third component to our mixture, much broader peak.  The idea is that when the sampler proposes jumping from one peak into the valley between the peaks, that valley isn't so deep, so the jump might get accepted.  You can do this by adding a third likelihood component -- maybe like this:</b>\n",
    "\n",
    "<pre>\n",
    "# Instead of...\n",
    "#loglike = sum(logaddexp(log(frac1) + loglike1, log(frac2) + loglike2))\n",
    "# Try...\n",
    "# A broad peak -- its width \"sigma\" is 5 units.\n",
    "loglike3 = - 0.5 .* ((a - (a1+a2)/2).^2 + (b - (b1+b2)/2).^2)./5^2\n",
    "# Add the first two components...\n",
    "loglike = logaddexp(log(frac1) + loglike1, log(frac2) + loglike2)\n",
    "# Add the third component, with weight 1%\n",
    "loglike = logaddexp(loglike, log(0.01) + loglike3)\n",
    "loglike = sum(loglike)\n",
    "</pre>\n",
    "\n",
    "<b>Does that change how balanced the peaks are?  Does it affect your sampling?  What happens if you make the broad peak broader?  What happens if you make it totally flat?  (ie, just set it to a constant value, like -8?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a funny-looking plot you can make that may help you think about the questions above.\n",
    "# You can plot a walker's \"track\" to see where it goes as the sampling proceeds.\n",
    "# \n",
    "f = Figure()\n",
    "Axis(f[1,1], xlabel=\"Step number\", ylabel=\"B\")\n",
    "nw = size(chain,2)\n",
    "nsam = size(chain,3)\n",
    "# Find one walker that, on average, was in B > 5\n",
    "for i in 1:nw\n",
    "    if mean(chain[1,i,:]) > 5\n",
    "        WGLMakie.lines!(1:nsam, chain[1,i,:])\n",
    "        break\n",
    "    end\n",
    "end\n",
    "# And find one that was in B < 5\n",
    "for i in 1:nw\n",
    "    if mean(chain[1,i,:]) < 5\n",
    "        WGLMakie.lines!(1:nsam, chain[1,i,:])\n",
    "        break\n",
    "    end\n",
    "end\n",
    "# Also plot one with large standard deviation -- ie likely jumped.\n",
    "for i in 1:nw\n",
    "    if std(chain[1,i,:]) > 3\n",
    "        WGLMakie.lines!(1:nsam, chain[1,i,:])\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "# In this plot, each walker is a different color, and you should see one or more of them jumping from one mode to the other.\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you plot *all* the walkers, you get a hairball of a plot, but you can see how many times they jump between the modes!\n",
    "f = Figure()\n",
    "Axis(f[1,1], xlabel=\"Step number\", ylabel=\"B\")\n",
    "nw = size(chain,2)\n",
    "nsam = size(chain,3)\n",
    "for i in 1:nw\n",
    "    WGLMakie.lines!(1:nsam, chain[1,i,:])\n",
    "end\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-ecology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-adrian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
