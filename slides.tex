\pdfobjcompresslevel=1
\documentclass{beamer}
\usepackage{pdfpages}
\usepackage{mathtools}
%\usepackage{amsmath}
\usepackage{tikz}
%\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,placments,fit}
\usetikzlibrary{arrows.meta,decorations.pathmorphing,backgrounds,positioning,fit}

%\usepackage[wby]{callouts}

\newcommand{\dfmpage}[1]{
{
\setbeamercolor{background canvas}{bg=}
\includepdf[pages=#1]{dfm.pdf}
}
}

\input{header}
\usefonttheme[onlymath]{serif}
\usepackage{multimedia} 

\newcommand{\niceurl}[1]{\mbox{\href{#1}{\textsl{#1}}}}

\title{Markov Chain Monte Carlo}
\author{Dustin Lang \\
Perimeter Institute for Theoretical Physics}
\date{Symmetries Graduate School, 2023-01-23 \\
  \vspace{1em}
Borrowing heavily from Dan Foreman-Mackey's slides \niceurl{https://speakerdeck.com/dfm/data-analysis-with-mcmc1}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\dfmpage{1}

\dfmpage{11}
\dfmpage{13}
\dfmpage{14}
%\dfmpage{16}

\begin{frame}{An example}
  \begin{overlayarea}{\textwidth}{0.4\textheight}
  \begin{itemize}
  \only<1>{
  \item Perlmutter+1999 (\niceurl{https://arxiv.org/abs/astro-ph/9812133})
  \item Measured the observed peak brightnesses of a sample of type-1a supernovae (in astronomer ``mag'' units), and the redshifts (``z'') of the supernova host galaxies
  \item $\textrm{mag} = \textrm{mag}_{\textrm{intrinsic}} + \textrm{luminosity\_distance}(z, \textrm{parameters}) + \epsilon$
  }%
  \only<2>{
  \item Generative model:
  \item $\textrm{mag} = \textrm{mag}_{\textrm{intrinsic}} + \textrm{luminosity\_distance}(z, \textrm{parameters}) + \epsilon$
  \item Probability of data given a model (``likelihood''):
\item $p(\textrm{data} \,|\, \textrm{parameters}) = \textrm{Gaussian}(\textrm{data} \,|\, \mu = f(\textrm{parameters}), \sigma^2)$
  \item $p(\textrm{mag} \,|\, \Omega_M, \Omega_{\Lambda} ) = \mathcal{N}(\textrm{mag} \,|\, \textrm{mag}_{\textrm{int}} + D_L(z, \Omega_M, \Omega_{\Lambda}), \sigma^2)$
  }%
  \only<3>{
  \item Use Bayes' theorem to put contraints on $\theta = \{ \Omega_M, \Omega_{\Lambda} \}$
  \item $p(\theta \,|\, \textrm{data}) \propto p(\theta) \, p(\textrm{data} \,|\, \theta)$
  }%
  \end{itemize}
  \end{overlayarea}
  % Figures generated by
  %https://colab.research.google.com/drive/1eQSVCxpXbed8sL6iufHAjprpgsOdcU8g#scrollTo=-Hwk9Jy1VvDg
  \only<1>{\includegraphics[height=0.4\textwidth]{pm1}}%
  \only<2>{\includegraphics[height=0.4\textwidth]{pm2}}%
  \only<3>{\includegraphics[height=0.4\textwidth]{pm3}}%
\end{frame}

\begin{frame}{An example}
  \begin{itemize}
    \item Resulting parameter constraints (blue ellipse):
  \end{itemize}
  \includegraphics[height=0.6\textwidth]{pm-constraints}
\end{frame}

\begin{frame}{Why we often need MCMC}
  \begin{itemize}
  % \item We want to put \alert{constraints on parameters} of a physical model
  %   \alert{based on observations}
  % \item constraints = posterior $= p( \textrm{parameters} | \textrm{data} )$ \\
  %   %\hspace{3em}
  %   %\small{(``The stellar mass of the Andromeda galaxy is $10 \pm 2 \times 10^{10} \textrm{M}_{\odot}$'')}
  %   \small{``The matter content of the universe (assuming flat $\Lambda$CDM) is $\Omega_{M} = 0.28 \pm 0.09$''}
  % \item $\propto \textrm{prior} \times \textrm{likelihood}$
  % \item $\propto p( \textrm{parameters} ) \times p( \textrm{data} | \textrm{parameters} )$
  % %\item We've seen examples with \alert{linear models} and \alert{Gaussian} likelihoods
  % %\item \cdots which can be solve using linear algebra
  \item Real-life models and likelihoods are often complex
  \item $\ldots$ so the resulting \alert{constraints} have complicated distributions (not Gaussians!)
    %\item We want to be able to \alert{marginalize} over ``nuisance'' parameters
  \item $\ldots$ but we can represent them with \alert{samplings}
  \item MCMC is good for drawing samples from complicated probability distributions
  \end{itemize}
\end{frame}

\begin{frame}{Samplings to represent constraints - examples}
  \includegraphics[height=0.5\textwidth]{corner}
  \begin{itemize}
  \item From https://arxiv.org/abs/1910.04899
  \item With a sampling: \alert{Marginalize} over a parameter by projecting it out
  \end{itemize}
\end{frame}

\begin{frame}{Samplings to represent constraints - examples}
  \includegraphics[height=0.5\textwidth]{banana}
  \begin{itemize}
    \item From https://arxiv.org/abs/1611.00036
  \end{itemize}
\end{frame}

\dfmpage{30-34}

\dfmpage{35}
% {
% \setbeamercolor{background canvas}{bg=}
% \includepdf[pages=35,pagecommand={%
% \begin{tikzpicture}%
% (0,0) node(x) {Hello World!};
% \end{tikzpicture}}]{dfm.pdf}

\dfmpage{36-45}

%\dfmpage{16}

\begin{frame}{About the name}
\begin{itemize}
\item \alert{Monte Carlo}: reference to the famous Monte Carlo Casino in Monaco, alluding to the randomness used in the algorithm
\item \alert{Markov Chain}: a set of samples, where each one is generated by a process that only looks at the previous one.
\item \alert{Markov}: a 19th-centure Russian mathematician and impressive-moustache-haver
with an \href{https://en.wikipedia.org/wiki/List_of_things_named_after_Andrey_Markov}{\textcolor{blue}{extensive list of things named after him}}
\item \alert{Metropolis--Hastings}: lead authors of 1953 and 1970 papers (resp.) giving the algorithm with symmetric and general proposal distributions (resp.)
\end{itemize}
\end{frame}

\begin{frame}{The Algorithm}
\end{frame}

\dfmpage{46-54}

\end{document}

