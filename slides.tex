\pdfobjcompresslevel=1
\documentclass{beamer}
\usepackage{pdfpages}
\usepackage{mathtools}
%\usepackage{amsmath}
\usepackage{tikz}
%\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,placments,fit}
\usetikzlibrary{arrows.meta,decorations.pathmorphing,backgrounds,positioning,fit}

\usepackage{minted}

%\usepackage[wby]{callouts}

\newcommand{\dfmpage}[1]{
{
\setbeamercolor{background canvas}{bg=}
\includepdf[pages=#1]{dfm.pdf}
}
}

\input{header}
\usefonttheme[onlymath]{serif}
\usepackage{multimedia} 

\newcommand{\niceurl}[1]{\mbox{\href{#1}{\textsl{#1}}}}

\title{Markov Chain Monte Carlo}
\author{Dustin Lang \\
Perimeter Institute for Theoretical Physics}
\date{Symmetries Graduate School, 2023-01-23 \\
  \vspace{1em}
Borrowing heavily from Dan Foreman-Mackey's slides \niceurl{https://speakerdeck.com/dfm/data-analysis-with-mcmc1}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\dfmpage{1}

\dfmpage{11}
\dfmpage{13}
\dfmpage{14}
%\dfmpage{16}

\begin{frame}{An example}
  \begin{overlayarea}{\textwidth}{0.4\textheight}
  \begin{itemize}
  \only<1>{
  \item Perlmutter+1999 (\niceurl{https://arxiv.org/abs/astro-ph/9812133})
  \item Measured the observed peak brightnesses of a sample of type-1a supernovae (in astronomer ``mag'' units), and the redshifts (``z'') of the supernova host galaxies
  \item $\textrm{mag} = \textrm{mag}_{\textrm{intrinsic}} + \textrm{luminosity\_distance}(z, \textrm{parameters}) + \epsilon$
  }%
  \only<2>{
  \item Generative model: \\
   $\textrm{mag}_i = \textrm{mag}_{\textrm{intrinsic}} + \textrm{luminosity\_distance}(z_i, \textrm{parameters}) + \epsilon_i$
  \item Probability of data given a model (``likelihood''): \\
   \small{$p(\{ \textrm{mag}_i \} \,|\, \textrm{params}) = \prod_i \textrm{Gaussian}(\textrm{mag}_i \,|\, \mu = f(z_i, \textrm{params}), \sigma_i^2)$}
  \item $p(\textrm{mag}_i \,|\, \Omega_M, \Omega_{\Lambda} ) = \mathcal{N}(\textrm{mag}_i \,|\, \textrm{mag}_{\textrm{int}} + D_L(z_i, \Omega_M, \Omega_{\Lambda}), \sigma_i^2)$
  }%
  \only<3>{
  \item Use Bayes' theorem to convert data likelihoods into contraints on the model parameters $\theta = \{ \Omega_M, \Omega_{\Lambda} \}$
  \item $p(\theta \,|\, \textrm{data}) \propto p(\theta) \, p(\textrm{data} \,|\, \theta)$
  \item $p(\Omega_M, \Omega_{\Lambda} \,|\, \{ \textrm{mag}_i \}) \propto$ \\
  $\quad p(\Omega_M, \Omega_{\Lambda}) \, \prod_i \mathcal{N}( \textrm{mag}_i \,|\, \textrm{mag}_{\textrm{int}} + D_L(z_i, \Omega_M, \Omega_{\Lambda}), \sigma_i^2)$
  }%
  \end{itemize}
  \end{overlayarea}
  \vspace{-1em}
  % Figures generated by
  %https://colab.research.google.com/drive/1eQSVCxpXbed8sL6iufHAjprpgsOdcU8g#scrollTo=-Hwk9Jy1VvDg
  \only<1>{\includegraphics[height=0.45\textwidth]{pm1}}%
  \only<2>{\includegraphics[height=0.45\textwidth]{pm2}}%
  \only<3>{\includegraphics[height=0.45\textwidth]{pm3}}%
\end{frame}

\begin{frame}{An example}
  \begin{itemize}
    \item Resulting parameter constraints (blue ellipse):
  \end{itemize}
  \includegraphics[height=0.6\textwidth]{pm-constraints}
\end{frame}

\begin{frame}{Why we often need MCMC}
  \begin{itemize}
  \item We want to put \alert{constraints on the parameters} of a physical model
    \alert{based on observations}
  % \item constraints = posterior $= p( \textrm{parameters} | \textrm{data} )$ \\
  %   %\hspace{3em}
  %   %\small{(``The stellar mass of the Andromeda galaxy is $10 \pm 2 \times 10^{10} \textrm{M}_{\odot}$'')}
  %   \small{``The matter content of the universe (assuming flat $\Lambda$CDM) is $\Omega_{M} = 0.28 \pm 0.09$''}
  % \item $\propto \textrm{prior} \times \textrm{likelihood}$
  % \item $\propto p( \textrm{parameters} ) \times p( \textrm{data} | \textrm{parameters} )$
  % %\item We've seen examples with \alert{linear models} and \alert{Gaussian} likelihoods
  % %\item \cdots which can be solve using linear algebra
  \item Real-life models and likelihoods are often complex
  \item $\ldots$ so the resulting \alert{constraints} have complicated distributions (not Gaussians!)
    %\item We want to be able to \alert{marginalize} over ``nuisance'' parameters
  \item $\ldots$ but we can represent them with \alert{samplings}
  \item MCMC is good for drawing samples from probability distributions
        that we can compute numerically but cannot solve analytically
  \end{itemize}
\end{frame}

\begin{frame}{Samplings to represent constraints - examples}
  \includegraphics[height=0.5\textwidth]{corner}
  \begin{itemize}
  \item From https://arxiv.org/abs/1910.04899
  \item With a sampling: \alert{Marginalize} over a parameter by projecting it out
  \end{itemize}
\end{frame}

\begin{frame}{Samplings to represent constraints - examples}
  %\includegraphics[height=0.5\textwidth]{banana}
  \includegraphics[height=0.5\textwidth]{fnl}
  \begin{itemize}
    \item From https://arxiv.org/abs/1611.00036
  \end{itemize}
\end{frame}

\dfmpage{30-34}

\dfmpage{35}
% {
% \setbeamercolor{background canvas}{bg=}
% \includepdf[pages=35,pagecommand={%
% \begin{tikzpicture}%
% (0,0) node(x) {Hello World!};
% \end{tikzpicture}}]{dfm.pdf}

\dfmpage{36-45}

%\dfmpage{16}

\begin{frame}{About the name}
\begin{itemize}
\item \alert{Monte Carlo}: a reference to the famous Monte Carlo Casino in Monaco, alluding to the randomness used in the algorithm
\item \alert{Markov Chain}: a list of samples, where each one is generated by a process that only looks at the previous one.
\item \alert{Markov}: a 19th-centure Russian mathematician and impressive-moustache-haver
with an \href{https://en.wikipedia.org/wiki/List_of_things_named_after_Andrey_Markov}{\textcolor{blue}{extensive list of things named after him}}
\item \alert{Metropolis--Hastings}: lead authors of 1953 and 1970 papers (resp.) giving the algorithm with symmetric and general proposal distributions (resp.)
\end{itemize}
\end{frame}

% \begin{frame}[containsverbatim]{The Algorithm}
% \begin{small}
% \begin{minted}{python}
% def mcmc(prob_func, propose_func, initial_pos, nsteps):
%      p = initial_pos
%      prob = prob_func(p)
%      chain = []
%      for i in range(nsteps):
%          # propose a new position in parameter space
%          p_new = propose_func(p)
%          # compute probability at new position
%          prob_new = prob_func(p_new)
%          # decide whether to jump to the new position
%          #...
%          # save the position
%          chain.append(p)
%      return chain
% \end{minted}
% \end{small}
% \end{frame}

\begin{frame}[fragile]{The Algorithm (1)}
\begin{small}
\begin{minted}{python}
def mcmc(prob_func, propose_func, initial_pos, nsteps):
     p = initial_pos
     prob = prob_func(p)
     chain = []
     for i in range(nsteps):
         # propose a new position in parameter space
         # ...
         # compute probability at new position
         # ...
         # decide whether to jump to the new position
         if # ...
             # ...
             # ...
         # save the position
         chain.append(p)
     return chain
\end{minted}
\end{small}
\end{frame}

\begin{frame}[fragile]{The Algorithm (2)}
\begin{small}
\begin{minted}{python}
def mcmc(prob_func, propose_func, initial_pos, nsteps):
     p = initial_pos
     prob = prob_func(p)
     chain = []
     for i in range(nsteps):
         # propose a new position in parameter space
         p_new = propose_func(p)
         # compute probability at new position
         prob_new = prob_func(p_new)
         # decide whether to jump to the new position
         if prob_new / prob > uniform_random():
             p = p_new
             prob = prob_new
         # save the position
         chain.append(p)
     return chain
\end{minted}
\end{small}
\end{frame}

\begin{frame}[fragile]{The Algorithm (3)}
\begin{small}
\begin{minted}{python}
def mcmc(logprob_func, propose_func, initial_pos, nsteps):
     p = initial_pos
     logprob = logprob_func(p)
     chain = []
     for i in range(nsteps):
         # propose a new position in parameter space
         p_new = propose_func(p)
         # compute probability at new position
         logprob_new = logprob_func(p_new)
         # decide whether to jump to the new position
         if exp(logprob_new - logprob) > uniform_random():
             p = p_new
             logprob = logprob_new
         # save the position
         chain.append(p)
     return chain
\end{minted}
\end{small}
\end{frame}

\begin{frame}[fragile]{The Algorithm (4)}
\begin{small}
\begin{minted}{python}
def mcmc(logprob_func, propose_func, initial_pos, nsteps):
     p = initial_pos
     logprob = logprob_func(p)
     chain = []
     naccept = 0
     for i in range(nsteps):
         # propose a new position in parameter space
         p_new = propose_func(p)
         # compute probability at new position
         logprob_new = logprob_func(p_new)
         # decide whether to jump to the new position
         if exp(logprob_new - logprob) > uniform_random():
             p = p_new
             logprob = logprob_new
             naccept += 1
         # save the position
         chain.append(p)
     return chain, naccept/nsteps
\end{minted}
\end{small}
\end{frame}

%
%
%

\begin{frame}{Conclusions}
\begin{itemize}
\item MCMC remains an essential tool for probabilistic inference
\item For science: lets us contrain model parameters based on data (Bayesian inference)
\item Beguilingly simple algorithm (with some pitfalls!)
\item A good proposal function can be hard to come up with! (and is essential for good performance!)
\item MCMC has beautiful theoretical guarantees... as compute time $\to \infty$
\end{itemize}
\end{frame}


\dfmpage{46-54}

\dfmpage{59-62}

\begin{frame}{A connection to symmetries}
\begin{itemize}
\item In Metropolis--Hastings MCMC, it's a major issue that you have to choose a good
proposal distribution---which requires a large number of tuning
parameters as the dimensionality of your problem increases
\item You can see this as a lack of \alert{symmetry} in the algorithm---the algorithm is
very sensitive to the parameterization of the problem
\item For example, it's not invariant to an \alert{affine} transformation
\item \alert{Next lecture}, I'll show you an alternative algorithm that \alert{does} have affine invariance
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{hardeasy}
\end{center}
\end{frame}




\begin{frame}{This afternoon's tutorial/lab session}
\begin{itemize}
\item Bob Room, 3:15--4:30
\item Time to play with MCMC yourself!
\item We'll use Google CoLab - no need to install anything on your computer
\item In the Python language
\end{itemize}
\end{frame}




\end{document}

